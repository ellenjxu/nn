{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement encoder + decoder! (Use cross attention with the KQ, and don't mask in encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\\n',\n",
       " 'Two young, White males are outside near many bushes.\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string level instead of char level generation\n",
    "with open('train.de', 'r', encoding='utf-8') as f:\n",
    "  de_text = f.readlines()\n",
    "with open('train.en', 'r', encoding='utf-8') as f:\n",
    "  en_text = f.readlines()\n",
    "  \n",
    "de_text[0], en_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "tensor([   27,  2257,  7227,    29,  1168, 42990, 10891,   469,   356,    72,\n",
      "        39683,    68,   337, 11033,    77,  1008,   264,   521,   545,  4848,\n",
      "         2013,   287,  4587,   399, 11033,   258,   410,  8207,   263,   347,\n",
      "         9116, 15952, 29847, 10619,    29])\n"
     ]
    }
   ],
   "source": [
    "# get tokens\n",
    "enc = tk.get_encoding('gpt2')\n",
    "vocab_size = enc.max_token_value + 1\n",
    "print(vocab_size)\n",
    "\n",
    "# <START> and <END> tokens\n",
    "de = [torch.tensor(enc.encode('<START> ' + d.replace('\\n', '<END>'))) for d in de_text]\n",
    "en = [torch.tensor(enc.encode('<START> ' + e.replace('\\n', '<END>'))) for e in en_text]\n",
    "\n",
    "print(de[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "tensor([   27,  2257,  7227,    29,  1168, 42990, 10891,   469,   356,    72,\n",
      "        39683,    68,   337, 11033,    77,  1008,   264,   521,   545,  4848,\n",
      "         2013,   287,  4587,   399, 11033,   258,   410,  8207,   263,   347,\n",
      "         9116, 15952, 29847, 10619,    29,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "# add padding to make all de sequences the same length\n",
    "\n",
    "# get max length\n",
    "max_len = max([len(d) for d in de])\n",
    "print(max_len)\n",
    "\n",
    "# pad\n",
    "de = [F.pad(d, (0, max_len - len(d))) for d in de]\n",
    "print(de[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<END>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode('<END>')\n",
    "enc.decode([27,10619,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle sentences and split into train and val\n",
    "import random\n",
    "\n",
    "z = list(zip(de, en))\n",
    "random.shuffle(z)\n",
    "\n",
    "# split\n",
    "n = int(0.9*len(z))\n",
    "train_data = z[:n]\n",
    "val_data = z[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   27,  2257,  7227,    29,  1168, 42990, 10891,   469,   356,    72,\n",
      "        39683,    68,   337, 11033,    77,  1008,   264,   521,   545,  4848,\n",
      "         2013,   287,  4587,   399, 11033,   258,   410,  8207,   263,   347,\n",
      "         9116, 15952, 29847, 10619,    29,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474,   867, 37413, 29847, 10619,    29]))\n",
      "tensor([ 2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,  2354,\n",
      "         1474,   867, 37413, 29847, 10619,    29])\n",
      "when input is tensor([27]) the target is 2257\n",
      "when input is tensor([  27, 2257]) the target is 7227\n",
      "when input is tensor([  27, 2257, 7227]) the target is 29\n",
      "when input is tensor([  27, 2257, 7227,   29]) the target is 4930\n",
      "when input is tensor([  27, 2257, 7227,   29, 4930]) the target is 1862\n",
      "when input is tensor([  27, 2257, 7227,   29, 4930, 1862]) the target is 11\n",
      "when input is tensor([  27, 2257, 7227,   29, 4930, 1862,   11]) the target is 2635\n",
      "when input is tensor([  27, 2257, 7227,   29, 4930, 1862,   11, 2635]) the target is 10835\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835]) the target is 389\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389]) the target is 2354\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354]) the target is 1474\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474]) the target is 867\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474,   867]) the target is 37413\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474,   867, 37413]) the target is 29847\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474,   867, 37413, 29847]) the target is 10619\n",
      "when input is tensor([   27,  2257,  7227,    29,  4930,  1862,    11,  2635, 10835,   389,\n",
      "         2354,  1474,   867, 37413, 29847, 10619]) the target is 29\n"
     ]
    }
   ],
   "source": [
    "# example of training data\n",
    "# x is (de, en) pairs\n",
    "# y is en_labels, next token in the en sentence\n",
    "\n",
    "x = (de[0], en[0]) # first sentence\n",
    "y = en[0][1:]\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "for t in range(len(x[1])-1):\n",
    "  context = x[1][:t+1]\n",
    "  target = y[t]\n",
    "  print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# batch_size = 8\n",
    "block_size = 8 # 64\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 1 \n",
    "n_embd = 64 \n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "Input: German sentence (prompt) and English sentence (this part needs block size and masking)\n",
    "Output: English sentence (next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  # generate a small batch of data of inputs x and targets y\n",
    "  \n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(data), (batch_size,)) # randomly select batch_size sentences\n",
    "  de = torch.stack([data[i][0] for i in ix])\n",
    "  en = [data[i][1] for i in ix]\n",
    "  en_labels = [data[i][1][1:] for i in ix]\n",
    "  \n",
    "  # randomly select block_size tokens from each sentence\n",
    "  # use same random indices for en and en_labels\n",
    "  en_b = []\n",
    "  en_labels_b = []\n",
    "  for s1, s2 in zip(en, en_labels):\n",
    "    i = torch.randint(len(s1)-block_size, (1,))[0]\n",
    "    en_b.append(s1[i:i+block_size])\n",
    "    en_labels_b.append(s2[i:i+block_size])\n",
    "  \n",
    "  en_b = torch.stack(en_b)\n",
    "  en_labels_b = torch.stack(en_labels_b)\n",
    "  \n",
    "  # # create block sizes for each en sentence and concatenate\n",
    "  # en_b = torch.stack([sentence[i:i+block_size] for sentence in en for i in range(0, len(sentence)-block_size, block_size)])\n",
    "  # en_labels_b = torch.stack([sentence[i:i+block_size] for sentence in en_labels for i in range(0, len(sentence)-block_size, block_size)])\n",
    "  \n",
    "  return (de.to(device), en_b.to(device)), en_labels_b.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "when input is [27] the target is 2257\n",
      "when input is [27, 2257] the target is 7227\n",
      "when input is [27, 2257, 7227] the target is 29\n",
      "when input is [27, 2257, 7227, 29] the target is 317\n",
      "when input is [27, 2257, 7227, 29, 317] the target is 38042\n",
      "when input is [27, 2257, 7227, 29, 317, 38042] the target is 7720\n",
      "when input is [27, 2257, 7227, 29, 317, 38042, 7720] the target is 5916\n",
      "when input is [27, 2257, 7227, 29, 317, 38042, 7720, 5916] the target is 284\n",
      "when input is [27] the target is 2257\n",
      "when input is [27, 2257] the target is 7227\n",
      "when input is [27, 2257, 7227] the target is 29\n",
      "when input is [27, 2257, 7227, 29] the target is 317\n",
      "when input is [27, 2257, 7227, 29, 317] the target is 1448\n",
      "when input is [27, 2257, 7227, 29, 317, 1448] the target is 286\n",
      "when input is [27, 2257, 7227, 29, 317, 1448, 286] the target is 30303\n",
      "when input is [27, 2257, 7227, 29, 317, 1448, 286, 30303] the target is 1627\n",
      "when input is [21671] the target is 257\n",
      "when input is [21671, 257] the target is 6512\n",
      "when input is [21671, 257, 6512] the target is 15207\n",
      "when input is [21671, 257, 6512, 15207] the target is 503\n",
      "when input is [21671, 257, 6512, 15207, 503] the target is 286\n",
      "when input is [21671, 257, 6512, 15207, 503, 286] the target is 4898\n",
      "when input is [21671, 257, 6512, 15207, 503, 286, 4898] the target is 29847\n",
      "when input is [21671, 257, 6512, 15207, 503, 286, 4898, 29847] the target is 10619\n",
      "when input is [10147] the target is 290\n",
      "when input is [10147, 290] the target is 257\n",
      "when input is [10147, 290, 257] the target is 2415\n",
      "when input is [10147, 290, 257, 2415] the target is 287\n",
      "when input is [10147, 290, 257, 2415, 287] the target is 257\n",
      "when input is [10147, 290, 257, 2415, 287, 257] the target is 20239\n",
      "when input is [10147, 290, 257, 2415, 287, 257, 20239] the target is 10147\n",
      "when input is [10147, 290, 257, 2415, 287, 257, 20239, 10147] the target is 290\n",
      "when input is [29] the target is 317\n",
      "when input is [29, 317] the target is 582\n",
      "when input is [29, 317, 582] the target is 287\n",
      "when input is [29, 317, 582, 287] the target is 257\n",
      "when input is [29, 317, 582, 287, 257] the target is 2042\n",
      "when input is [29, 317, 582, 287, 257, 2042] the target is 10147\n",
      "when input is [29, 317, 582, 287, 257, 2042, 10147] the target is 5586\n",
      "when input is [29, 317, 582, 287, 257, 2042, 10147, 5586] the target is 379\n",
      "when input is [10846] the target is 287\n",
      "when input is [10846, 287] the target is 257\n",
      "when input is [10846, 287, 257] the target is 2330\n",
      "when input is [10846, 287, 257, 2330] the target is 6873\n",
      "when input is [10846, 287, 257, 2330, 6873] the target is 1353\n",
      "when input is [10846, 287, 257, 2330, 6873, 1353] the target is 318\n",
      "when input is [10846, 287, 257, 2330, 6873, 1353, 318] the target is 6155\n",
      "when input is [10846, 287, 257, 2330, 6873, 1353, 318, 6155] the target is 4769\n",
      "when input is [661] the target is 287\n",
      "when input is [661, 287] the target is 511\n",
      "when input is [661, 287, 511] the target is 1542\n",
      "when input is [661, 287, 511, 1542] the target is 82\n",
      "when input is [661, 287, 511, 1542, 82] the target is 1011\n",
      "when input is [661, 287, 511, 1542, 82, 1011] the target is 257\n",
      "when input is [661, 287, 511, 1542, 82, 1011, 257] the target is 1448\n",
      "when input is [661, 287, 511, 1542, 82, 1011, 257, 1448] the target is 4590\n",
      "when input is [317] the target is 4950\n",
      "when input is [317, 4950] the target is 8408\n",
      "when input is [317, 4950, 8408] the target is 286\n",
      "when input is [317, 4950, 8408, 286] the target is 257\n",
      "when input is [317, 4950, 8408, 286, 257] the target is 4928\n",
      "when input is [317, 4950, 8408, 286, 257, 4928] the target is 379\n",
      "when input is [317, 4950, 8408, 286, 257, 4928, 379] the target is 26428\n",
      "when input is [317, 4950, 8408, 286, 257, 4928, 379, 26428] the target is 351\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "# print(xb[0][:5]) # xb[0] is the de sentences\n",
    "# print(xb[1][:5]) # xb[1] is the en sentences, corresponding to y\n",
    "# print(yb)\n",
    "\n",
    "print('---')\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[1][b,:t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "  def __init__(self, num_heads):\n",
    "      super().__init__()\n",
    "      self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "      self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "      self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "      self.proj = nn.Linear(n_embd, n_embd)\n",
    "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "      self.num_heads = num_heads\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "      k = self.split(self.key(k)) # (B, T, C) -> (B, H, T, C/H)\n",
    "      q = self.split(self.query(q))\n",
    "      # compute attention scores (\"affinities\") and normalize by head size\n",
    "      wei = q @ k.transpose(-2,-1) * (n_embd // self.num_heads) ** -0.5 # (B, H, T, C/H) @ (B, H, C/H, T) -> (B, H, T, T\n",
    "      if mask:\n",
    "        wei = wei.masked_fill(self.tril[:wei.shape[-2], :wei.shape[-1]] == 0, float('-inf')) # mask out future tokens\n",
    "      wei = F.softmax(wei, dim=-1) # (B, H, T, T)\n",
    "      wei = self.dropout(wei)\n",
    "      # perform the weighted aggregation of the values\n",
    "      v = self.split(self.value(v))\n",
    "      # use q below because q will always have the same shape as x\n",
    "      out = (wei @ v).transpose(1,2).contiguous().view(q.shape[0], q.shape[2], -1) # (B, H, T, T) @ (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\n",
    "      return out\n",
    "\n",
    "  def split(self, x): # split the last dimension into num_heads\n",
    "      B,T,C = x.shape\n",
    "      return x.view(B, T, self.num_heads, C // self.num_heads).transpose(1,2)\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        # head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.sa(q=x, k=x, v=x, mask=None) # residual skip connections +\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "      \n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        # head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head)\n",
    "        self.eda = MultiHeadAttention(n_head)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, enc):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.sa(q=x, k=x, v=x, mask=True)\n",
    "        # encoder-decoder attention\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.eda(q=x, k=enc, v=enc, mask=None)\n",
    "        x = self.ln3(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return (x, enc) # need to include enc for sequential (next block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Transformer encoder: a stack of Transformer blocks \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" Transformer decoder: a stack of Transformer blocks \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, enc):\n",
    "        for block in self.blocks:\n",
    "            x, enc = block(x, enc)\n",
    "        return x, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer model: encoder + decoder \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, max_len, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table_inpt = nn.Embedding(block_size, n_embd)\n",
    "        self.position_embedding_table_prompt = nn.Embedding(max_len, n_embd)\n",
    "        self.enc = Encoder(n_embd, n_head, n_layer)\n",
    "        self.dec = Decoder(n_embd, n_head, n_layer)\n",
    "        # self.enc = nn.Sequential(*[EncoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # self.dec = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        prompt, inpt = x\n",
    "        prompt = self.token_embedding_table(prompt) + self.position_embedding_table_prompt(torch.arange(prompt.shape[1], device=prompt.device))\n",
    "        inpt = self.token_embedding_table(inpt) + self.position_embedding_table_inpt(torch.arange(inpt.shape[1], device=inpt.device))\n",
    "        enc_out = self.enc(prompt)\n",
    "        dec_out, _ = self.dec(inpt, enc_out)\n",
    "        logits = self.lm_head(self.ln_f(dec_out))\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.954961 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(n_embd, max_len, n_head, n_layer).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.018996238708496\n"
     ]
    }
   ],
   "source": [
    "# for iter in range(max_iters):\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = model(xb, yb)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.040460586547852\n",
      "8.5200777053833\n"
     ]
    }
   ],
   "source": [
    "for iter in range(500): # max_iters\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = model(xb, yb)\n",
    "  \n",
    "  if iter % 100 == 0:\n",
    "    print(loss.item())\n",
    "  \n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14998\n",
      "tensor([   27,  2257,  7227,    29,   412,   259, 20291,  1976,   494,  4352,\n",
      "          304, 42326,   370, 11286,    11,  4587, 10255,   412,   320,  1142,\n",
      "           11, 24884,   354,    76,  8158,  3318, 30837,   268,   410,   692,\n",
      "          894, 40780,   318,    83, 29847, 10619,    29,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]) tensor([   27,  2257,  7227,    29,  1869, 10427,   257,  6383,  1336,   286,\n",
      "        38674,    11,   285,  2840,   290,  1379,  3150, 29847, 10619,    29]) tensor([ 2257,  7227,    29,  1869, 10427,   257,  6383,  1336,   286, 38674,\n",
      "           11,   285,  2840,   290,  1379,  3150, 29847, 10619,    29])\n"
     ]
    }
   ],
   "source": [
    "# try with just one sample\n",
    "\n",
    "# randomly get a sample\n",
    "idx = random.randint(0, len(train_data))\n",
    "print(idx)\n",
    "\n",
    "de_test = train_data[idx][0]\n",
    "en_test = train_data[idx][1]\n",
    "en_label_test = train_data[idx][1][1:]\n",
    "\n",
    "print(de_test, en_test, en_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(de_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional embedding size should actually be max_sentence_len + 1\n",
    "# for now just use the length of the sentence\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "position_embedding_table = nn.Embedding(len(de_test), n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3093,  0.7866, -0.5895,  ..., -1.3089,  0.2187,  0.7966],\n",
       "        [-3.4576, -0.7954, -2.3464,  ...,  1.1559,  0.0368,  0.0379],\n",
       "        [-2.5936,  1.0514, -0.8126,  ...,  3.7348,  1.2958, -2.9335],\n",
       "        ...,\n",
       "        [-2.7214, -1.6235,  0.1155,  ..., -2.2392, -1.1426, -0.8550],\n",
       "        [-0.7753, -1.3761,  0.8096,  ..., -2.9444, -0.5745, -1.4609],\n",
       "        [-3.3628, -0.5707,  2.4626,  ...,  0.4927, -0.8976,  0.6312]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_test = token_embedding_table(de_test)\n",
    "emb_test2 = position_embedding_table(torch.arange(de_test.shape[0])) # what position each word is in the sentence\n",
    "emb_test3 = emb_test + emb_test2\n",
    "\n",
    "emb_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_test.shape # makes sense because we are getting n_embd embedding for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([102, 64]), torch.Size([102, 64]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_test2.shape, emb_test3.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yayy encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 102, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_test3.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2188,  0.7534, -0.7414,  ..., -0.7603,  0.1801,  0.0924],\n",
       "         [-2.1068, -0.1372, -1.9280,  ...,  0.9991,  0.2325, -0.5434],\n",
       "         [-1.3446,  1.0218, -0.8601,  ...,  2.7342,  1.1625, -2.0194],\n",
       "         ...,\n",
       "         [-1.7012, -0.7692, -0.1831,  ..., -1.1328, -0.8862, -0.5705],\n",
       "         [-0.2505, -0.5427,  0.4150,  ..., -1.1798, -0.7398, -0.8358],\n",
       "         [-2.2766, -0.0478,  1.1928,  ...,  0.8402, -0.2884, -0.2580]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have an embedding, we can pass it through the encoder\n",
    "\n",
    "enc = EncoderBlock(n_embd, n_head)\n",
    "# add a dimension to the embedding because the encoder expects a batch of sentences\n",
    "enc_out_test = enc(emb_test3.unsqueeze(0))\n",
    "enc_out_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass through decoder now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   27,  2257,  7227,    29,  1869, 10427,   257,  6383]),\n",
       " tensor([ 2257,  7227,    29,  1869, 10427,   257,  6383,  1336]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now just crop en_test to block_size (actual transformer would use a sliding window)\n",
    "en_test = en_test[:block_size]\n",
    "en_label_test = en_label_test[:block_size]\n",
    "en_test, en_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding_table2 = nn.Embedding(block_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_emb1 = token_embedding_table(en_test)\n",
    "en_emb2 = position_embedding_table2(torch.arange(en_test.shape[0]))\n",
    "en_emb3 = en_emb1 + en_emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 64]), torch.Size([1, 102, 64]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb3.unsqueeze(0).shape, enc_out_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2742e-02, -6.6307e-01,  9.5389e-01,  6.4754e-01,  5.4297e-01,\n",
       "           1.1434e+00,  1.5338e+00, -2.6165e-01,  8.4796e-01,  5.3543e-02,\n",
       "          -2.2576e-01,  7.0200e-01, -3.3272e+00, -3.9943e-01, -2.0145e+00,\n",
       "           7.4581e-02, -1.2408e+00,  2.4673e-01, -5.0055e-01,  6.7559e-01,\n",
       "          -1.3922e+00,  2.2750e-01,  5.7054e-01,  1.0686e+00,  1.1364e+00,\n",
       "           1.5668e+00, -2.2240e-01, -1.6714e+00,  7.8573e-01,  1.4196e+00,\n",
       "           4.4317e-01, -1.5133e+00,  2.7186e-01, -1.1186e+00, -2.1555e+00,\n",
       "           1.5989e-01,  1.0834e+00,  1.3989e+00,  8.4593e-01,  1.0235e+00,\n",
       "           1.8332e-01,  8.6455e-01,  1.0189e-01, -8.9185e-01, -3.6967e-01,\n",
       "          -1.8523e+00, -1.3451e+00, -3.0293e-01, -2.2932e-01,  6.4974e-01,\n",
       "           8.3090e-01,  3.7256e-01, -1.7152e+00,  1.1702e+00, -8.9748e-01,\n",
       "           1.9070e-01,  1.3602e+00,  6.9795e-01, -1.0332e+00,  3.2601e-01,\n",
       "          -1.0430e-01, -2.9414e-01,  5.1320e-01,  8.8965e-01],\n",
       "         [-1.7029e+00,  8.7928e-01, -3.7458e-01, -1.1080e+00,  1.7253e+00,\n",
       "           2.4033e+00,  1.1168e+00, -1.2177e+00, -1.5930e+00, -9.9967e-01,\n",
       "          -1.3267e+00,  1.7934e-01,  4.0677e-01, -1.7516e+00, -3.2656e-02,\n",
       "           1.4350e+00,  5.5981e-01, -3.3761e-01,  7.1857e-02,  5.4152e-01,\n",
       "           7.7122e-01,  3.4796e-01, -1.3617e-02, -7.5215e-02, -7.1299e-01,\n",
       "           1.9895e+00,  1.0751e+00, -2.9826e-01,  3.7831e-01, -2.7220e-01,\n",
       "           7.6508e-01,  2.8557e-02,  1.5186e+00,  2.3358e-01, -2.0851e+00,\n",
       "           1.6498e-01, -4.4621e-01, -1.5028e+00, -9.9486e-02, -3.1801e-01,\n",
       "          -7.1663e-01,  1.9342e+00,  1.4308e-01,  1.1521e+00, -5.3521e-01,\n",
       "           1.2296e+00, -9.0002e-01, -9.7711e-01,  1.3983e-01, -8.5138e-01,\n",
       "          -1.9800e+00, -1.1985e-01,  7.4966e-01, -2.2458e-01,  1.8640e-02,\n",
       "          -2.5190e-01,  4.0468e-01, -4.0222e-01,  6.5537e-01, -1.3759e-01,\n",
       "          -5.9104e-01, -1.0502e-01,  1.8948e-01, -9.7617e-01],\n",
       "         [-1.2514e+00,  1.0293e+00, -1.6833e+00,  4.5440e-01,  1.9171e-01,\n",
       "           3.4602e-01,  7.4461e-01,  2.3933e-01, -3.6531e-01, -1.6458e+00,\n",
       "           9.4960e-01,  2.3713e+00, -7.3600e-01,  1.1599e-01,  8.1723e-01,\n",
       "          -6.5693e-01, -7.0829e-01, -6.0192e-01,  1.7471e-01,  8.3805e-01,\n",
       "          -2.1003e-01, -1.4445e-01,  1.0854e+00,  1.0498e+00,  2.2471e-01,\n",
       "           2.9349e+00, -3.8593e-03, -1.2801e+00,  4.2241e-01, -7.5027e-01,\n",
       "           1.5108e+00,  6.3691e-01, -9.5627e-02, -5.4877e-01, -1.7825e+00,\n",
       "           8.0701e-02, -5.3802e-01, -3.9863e-01,  3.5222e-03,  7.6820e-01,\n",
       "          -6.0076e-01,  5.5342e-02,  2.4584e-01,  3.7123e-01, -6.1469e-01,\n",
       "          -2.0237e-01,  9.9775e-01, -5.5140e-01,  1.4043e-01,  1.3885e+00,\n",
       "          -2.2146e+00, -1.2527e+00,  1.3548e-01, -1.5501e+00, -2.1206e+00,\n",
       "          -1.7016e+00,  1.0466e+00,  9.5564e-01,  1.4931e+00, -5.0568e-01,\n",
       "           1.9287e-01,  1.3430e+00,  3.9663e-01,  1.0603e+00],\n",
       "         [ 2.2745e-01, -5.3201e-01, -6.2534e-01, -1.7086e-01,  1.7275e+00,\n",
       "           2.8106e-01,  8.1927e-02, -2.8770e+00, -4.6883e-01, -5.6550e-01,\n",
       "           3.2431e-01,  8.3386e-01,  1.5366e-01,  4.8805e-01, -4.6754e-01,\n",
       "           5.1896e-01, -9.5578e-01, -2.6850e+00, -2.4265e-01, -9.5457e-03,\n",
       "          -9.3978e-02, -1.6835e+00,  3.6008e-01,  3.2141e-01,  1.4780e+00,\n",
       "          -9.3080e-01,  5.1401e-01, -9.5121e-01,  2.4026e-01, -5.3991e-01,\n",
       "          -4.4155e-01, -7.8258e-01, -1.3552e+00,  1.0603e+00,  2.3874e+00,\n",
       "          -8.3044e-01, -2.8052e-01,  6.6835e-01,  5.2607e-01, -2.1146e-01,\n",
       "          -2.6289e-01,  1.1941e+00, -1.7008e-01,  2.4122e+00,  7.9490e-01,\n",
       "           1.3541e+00,  1.8557e-01, -1.2538e+00, -3.6051e-01,  6.3847e-01,\n",
       "          -2.5783e+00,  8.8655e-01, -4.6650e-01, -1.2158e+00, -5.7495e-01,\n",
       "          -4.5897e-01,  1.5678e+00,  7.3898e-01, -6.9398e-01,  7.6160e-02,\n",
       "          -4.8585e-01,  4.7620e-01, -4.8044e-02,  1.7042e+00],\n",
       "         [-1.0291e+00, -8.4497e-01, -4.3852e-01,  2.6447e-01,  7.8556e-01,\n",
       "          -1.5600e+00,  8.2888e-02, -8.8256e-01, -8.7390e-01,  1.4023e+00,\n",
       "           1.2861e-01,  1.2343e+00,  5.7992e-01,  2.7538e-01,  1.7867e+00,\n",
       "          -3.5324e-01, -2.0658e+00, -1.6332e-01,  2.9436e-01,  1.0206e+00,\n",
       "          -6.1528e-01, -4.7311e-01,  8.4701e-02,  9.0568e-01,  1.6956e+00,\n",
       "           8.9676e-01,  4.2343e-02, -1.2827e-01, -1.2815e-01, -1.8689e+00,\n",
       "          -6.9224e-01, -1.6617e+00,  5.3955e-01,  1.6844e+00, -9.2236e-01,\n",
       "          -1.7470e-01,  8.8237e-01, -1.6064e-01,  2.0110e+00,  9.9730e-01,\n",
       "          -3.5511e-01,  8.5845e-01, -1.5872e+00,  8.4147e-01, -1.1934e+00,\n",
       "           4.7615e-01,  1.2107e+00,  1.3269e+00,  1.5998e+00, -8.1418e-01,\n",
       "          -1.0209e+00, -1.6404e+00, -9.5833e-01, -9.1527e-01, -2.5748e+00,\n",
       "           6.3720e-01, -2.2764e-02, -1.1146e+00,  7.9045e-01, -4.4474e-01,\n",
       "          -5.4658e-01,  1.8274e+00, -2.3827e-01,  9.6565e-01],\n",
       "         [ 7.4206e-01,  1.0704e-01,  1.1039e+00, -1.4685e-01,  1.9416e-01,\n",
       "           1.5837e+00,  1.2971e+00, -7.7067e-01,  3.1522e-01, -8.3953e-01,\n",
       "          -7.4689e-01, -1.3160e-01,  1.7152e+00, -3.3255e-01,  1.3878e+00,\n",
       "           7.1687e-01, -3.6097e+00, -1.7984e+00, -1.6891e-01,  1.1016e+00,\n",
       "           9.1059e-01,  2.6488e-01,  4.4535e-01, -2.9326e-01, -7.0575e-01,\n",
       "           4.8468e-01, -1.2192e+00, -5.6944e-01, -7.0809e-01, -1.8843e+00,\n",
       "          -7.7864e-01, -1.1712e+00,  3.9414e-01, -3.4073e-01,  6.9064e-01,\n",
       "          -1.6347e+00,  7.7677e-01, -8.2158e-01,  2.3332e-01, -1.0712e+00,\n",
       "          -1.1143e-01,  1.4768e+00, -3.9485e-01,  8.5173e-01,  8.7032e-01,\n",
       "           8.2461e-01,  3.2531e-01,  1.2517e+00, -3.7547e-01, -1.4656e+00,\n",
       "          -2.3975e-01, -2.9776e-01,  1.9503e-01, -1.3375e+00,  1.6786e+00,\n",
       "          -1.0283e+00, -1.9986e-01, -7.1323e-01,  2.4210e+00, -1.9718e+00,\n",
       "           2.1912e-03,  5.0625e-01, -1.3366e+00,  9.9648e-01],\n",
       "         [ 1.6666e+00,  1.1941e+00, -1.2578e+00,  1.6046e+00, -7.8601e-01,\n",
       "          -1.0162e+00, -8.8459e-02, -4.0798e+00,  1.4341e+00, -4.8463e-01,\n",
       "           3.7419e-01, -1.1814e-01,  1.3144e+00,  1.2121e+00, -5.6284e-01,\n",
       "          -1.9915e+00, -1.3828e-01, -1.7693e+00,  7.9141e-01,  1.2323e+00,\n",
       "           5.8703e-02,  1.0873e+00,  1.6891e+00,  8.2088e-01, -1.9397e-01,\n",
       "           8.1226e-01,  3.7456e-01, -6.1475e-01,  6.3733e-01,  7.1632e-01,\n",
       "           1.4763e+00, -9.7624e-01, -2.1255e+00, -5.9320e-01,  3.1429e-01,\n",
       "          -7.8528e-01,  1.8456e+00, -1.2075e-01,  1.2465e+00, -1.1708e+00,\n",
       "          -1.0915e+00, -3.1033e-01,  3.6192e-01,  5.2093e-01,  8.4262e-01,\n",
       "           3.5736e-01, -6.6596e-01, -3.0886e-01, -3.2365e-01, -1.4772e+00,\n",
       "          -1.1498e+00, -2.7652e-01, -3.7276e-01,  2.3105e-01, -1.3961e+00,\n",
       "          -6.4453e-01,  2.9253e-01,  7.3303e-01,  1.1287e+00, -7.4064e-01,\n",
       "          -3.1077e-01,  1.7231e-01, -1.2174e-01,  6.0044e-01],\n",
       "         [-1.5640e+00, -4.0434e-01, -3.1038e-01,  1.2958e+00,  2.0741e-01,\n",
       "           7.6906e-01, -1.9280e+00, -1.0111e+00,  1.0889e+00, -5.9732e-03,\n",
       "           5.5962e-01,  5.4594e-01,  8.9271e-01,  4.4310e-02, -2.1302e+00,\n",
       "           2.0990e-01, -9.3171e-01,  1.4939e+00,  8.2691e-01,  1.3660e+00,\n",
       "           9.6468e-01, -5.5874e-01, -5.5435e-01, -2.2292e+00,  1.4166e+00,\n",
       "          -3.8775e-01, -1.4774e-01, -4.7817e-01,  1.1551e+00, -2.0804e-01,\n",
       "          -1.6935e+00, -3.0674e-01, -4.1516e-01,  8.4132e-01, -1.0225e+00,\n",
       "          -1.4206e-01,  2.8456e-01,  1.7507e+00,  1.2202e+00,  7.1523e-01,\n",
       "          -2.5291e+00,  1.6740e-02,  2.0869e-01,  9.7740e-01, -9.6776e-01,\n",
       "          -3.5974e-01, -9.7702e-02, -1.7496e-01,  4.8876e-01, -3.3828e-01,\n",
       "          -5.0214e-01, -1.8968e-01, -7.8186e-01,  1.7013e+00,  7.1555e-01,\n",
       "          -9.7590e-02,  1.2190e-01, -1.4538e+00, -3.8054e-01,  1.5929e+00,\n",
       "          -6.0661e-01,  9.3288e-01, -1.6430e+00,  1.5204e+00]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock(n_embd, n_head)\n",
    "dec_out_test = dec(en_emb3.unsqueeze(0), enc_out_test)\n",
    "dec_out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out_test.shape # logits are (B,T,vocab_size); in this case, (1, block_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (blocks): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (eda): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (eda): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (eda): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (eda): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get logits and evaluate loss (part of Transformer class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3545, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = nn.Linear(n_embd, vocab_size)(dec_out_test)\n",
    "targets = en_label_test\n",
    "loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1)) # some reshaping so that there is only 2 dims\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 50257]), torch.Size([8]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, targets.shape\n",
    "# makes sense because we are getting probabilities for each token in vocab being the next token and computing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29663, 16217, 10084, 23310, 46877, 11894, 14868, 27781]])\n",
      "tensor([ 2257,  7227,    29,  1869, 10427,   257,  6383,  1336])\n"
     ]
    }
   ],
   "source": [
    "# index into the highest probability token - these are the predictions!\n",
    "print(logits.argmax(dim=-1))\n",
    "print(targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate test (sample from model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m context\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(generate(model, prompt, context))\n",
      "\u001b[1;32m/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb Cell 46\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, prompt, context, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     logits, _ \u001b[39m=\u001b[39m model((prompt, context))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     logits \u001b[39m=\u001b[39m logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb Cell 46\u001b[0m in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m prompt, inpt \u001b[39m=\u001b[39m x\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(prompt) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table_prompt(torch\u001b[39m.\u001b[39marange(prompt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], device\u001b[39m=\u001b[39mprompt\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m inpt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(inpt) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embedding_table_inpt(torch\u001b[39m.\u001b[39;49marange(inpt\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], device\u001b[39m=\u001b[39;49minpt\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m enc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ellen/Documents/code/micrograd/6-transformer2.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m dec_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec(inpt, enc_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# what we feed into the model is german sentence and <START> token\n",
    "\n",
    "example_de = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen .\"\n",
    "prompt = torch.tensor(enc.encode('<START> ' + example_de + '<END>')).unsqueeze(0)\n",
    "context = torch.tensor(enc.encode('<START>')).unsqueeze(0)\n",
    "\n",
    "def generate(model, prompt, context, max_len=100):\n",
    "  with torch.no_grad():\n",
    "    for i in range(max_len):\n",
    "      logits, _ = model((prompt, context))\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      context = torch.cat((context.squeeze(0), next_token.squeeze(0)), dim=0).unsqueeze(0)\n",
    "      if next_token.item() == enc.encode('<END>'):\n",
    "        break\n",
    "  return context\n",
    "\n",
    "print(generate(model, prompt, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  27, 2257, 7227,   29]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   29,   257,   287,   257,   287,    29,   287, 29847])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, -1, :].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "696138aadfbb39da019b0f4a82f739b9650213a50720b5fab5db3742a6f6c84f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
